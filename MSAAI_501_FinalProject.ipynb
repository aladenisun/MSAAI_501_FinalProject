{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y62UtNwD-fE"
      },
      "source": [
        "### PYTHON LIBRARY SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "80I9s7Mqtqbf",
        "outputId": "35e4e796-0ada-474b-cbf6-5f16ea35d1ef"
      },
      "outputs": [],
      "source": [
        "# Import the ***TextBlob*** class for sentiment analysis and define a function to extract polarity and subjectivity from text, as specified in the instructions.\n",
        "!pip install textblob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNiLs4HzKPte"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Import the ***nltk*** library and download the necessary 'stopwords' and 'punkt' corpora for text preprocessing, as specified in the instructions.\n",
        "# This ensures that these resources are available for tokenization and stop word removal.\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqRmfpMFXCO7"
      },
      "source": [
        "### DOWNLOAD AND LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "e7uoSQlAwtLy",
        "outputId": "f22a5893-ee64-444c-c2cb-aef326463cca"
      },
      "outputs": [],
      "source": [
        "# Download Dataset\n",
        "path_fake_real = kagglehub.dataset_download(\n",
        "    \"clmentbisaillon/fake-and-real-news-dataset\"\n",
        ")\n",
        "print(\"fake/real path:\", path_fake_real)\n",
        "\n",
        "path_ai1 = kagglehub.dataset_download(\n",
        "    \"walidbenaouda/ai-isot-dataset\"\n",
        ")\n",
        "path_ai2 = kagglehub.dataset_download(\n",
        "    \"atharvasoundankar/gen-ai-misinformation-detection-datase-20242025\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sALHpa3TXXpd"
      },
      "outputs": [],
      "source": [
        "# Load the Fake and True CSV Datasets\n",
        "fake = pd.read_csv(os.path.join(path_fake_real, \"Fake.csv\")) #cvs for fake\n",
        "true = pd.read_csv(os.path.join(path_fake_real, \"True.csv\")) #cvs for true\n",
        "\n",
        "# Load AI Datasets\n",
        "ai_isot = pd.read_csv(os.path.join(path_ai1, \"AI-ISOT dataset.csv\"))\n",
        "ai_gen = pd.read_csv(os.path.join(path_ai2, \"generative_ai_misinformation_dataset.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "sEGKUTIJKXm4",
        "outputId": "f8fffc4c-23d2-40ff-de6f-44bd7e382ff3"
      },
      "outputs": [],
      "source": [
        "print(f\"-----------------------FAKE.CSV---------------------{fake.head()}\\n\") #shows headers\n",
        "print(f\"-----------------------TRUE.CSV---------------------{true.head()}\\n\") #shows headers\n",
        "print(f\"-----------------------AI_1.CSV----------------------{ai_isot.head()}\\n\")   #shows headers\n",
        "print(f\"-----------------------AI_2.CSV----------------------{ai_gen.head()}\\n\")   #shows headers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJRJaSgJb6iH"
      },
      "source": [
        "### TEXT CLEANING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvcvPt8AeatZ"
      },
      "source": [
        "#### Data Cleaning Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IHmbf4ub_Uv"
      },
      "outputs": [],
      "source": [
        "# Cleaning text in the dataset\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text) # Gets rid of urls\n",
        "    text = re.sub(r\"<.*?>\", \"\", text) # Gets rid of html\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?'â€™\\s]\", \" \", text) # Makes sure punctuation still there\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # Fixes spaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Pgxlp2cXN9"
      },
      "source": [
        "#### Clean Fake vs True Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "KLrI6srNKONT",
        "outputId": "c0a99b0e-30c0-4516-e1ef-06484a4adf67"
      },
      "outputs": [],
      "source": [
        "# Clean real vs fake dataset\n",
        "fake[\"is_real\"] = 0 # Gives fake news a label\n",
        "true[\"is_real\"] = 1 # Gives true news a label\n",
        "\n",
        "df_rf = pd.concat([fake, true], ignore_index=True) # Combines datasets\n",
        "\n",
        "# Makes text in one place\n",
        "df_rf[\"text\"] = (\n",
        "    df_rf[\"title\"].fillna(\"\") + \" \" +\n",
        "    df_rf[\"text\"].fillna(\"\")\n",
        ").str.strip()\n",
        "\n",
        "# Gets rid of duplicates and empty text\n",
        "df_rf = df_rf.drop_duplicates(subset=[\"text\"])\n",
        "df_rf = df_rf.dropna(subset=[\"text\"])\n",
        "df_rf = df_rf[df_rf[\"text\"].str.strip() != \"\"]\n",
        "# Cleans the text\n",
        "df_rf[\"text\"] = df_rf[\"text\"].apply(clean_text)\n",
        "\n",
        "# Picks the necessary columns and saves the cleaned file\n",
        "df_rf_clean = df_rf[[\"text\", \"is_real\"]]\n",
        "df_rf_clean[\"source\"] = \"FAKE-REAL\"\n",
        "df_rf_clean.to_csv(\"clean_real_fake.csv\", index=False)\n",
        "\n",
        "# Show it was saved and shows portion of cleaned data set\n",
        "print(\"saved: clean_real_fake.csv\")\n",
        "print(df_rf_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQ5mFkHcp-f"
      },
      "source": [
        "#### Clean AI vs Human Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wxRLhrcy1N"
      },
      "source": [
        "##### AI-Dataset 1 - AI-ISOT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "ae6v9nvyhHla",
        "outputId": "2fc958ea-307d-48bf-9560-c505bd932aee"
      },
      "outputs": [],
      "source": [
        "print(ai_isot.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "pWBlqFKYrY-s",
        "outputId": "aa326087-7d9e-45b4-8434-1991f1cc131a"
      },
      "outputs": [],
      "source": [
        "ai_isot_long = []\n",
        "\n",
        "# Human-written REAL news : human, real\n",
        "for x in ai_isot[\"Real News\"].dropna():\n",
        "    ai_isot_long.append({\"text\": x, \"is_ai\": 0, \"is_real\": 1})\n",
        "\n",
        "# Human-written FAKE news : human, fake\n",
        "for x in ai_isot[\"Fake News\"].dropna():\n",
        "    ai_isot_long.append({\"text\": x, \"is_ai\": 0, \"is_real\": 0})\n",
        "\n",
        "# AI-generated Fake News : ai, fake\n",
        "for x in ai_isot[\"AI-generated Fake News\"].dropna():\n",
        "    ai_isot_long.append({\"text\": x, \"is_ai\": 1, \"is_real\": 0})\n",
        "\n",
        "ai_isot_df = pd.DataFrame(ai_isot_long)\n",
        "\n",
        "ai_isot_df[\"text\"] = ai_isot_df[\"text\"].apply(clean_text)\n",
        "ai_isot_df[\"source\"] = \"AI-ISOT\"\n",
        "\n",
        "ai_isot_df.to_csv(\"clean_ai_isot.csv\", index=False)\n",
        "print(\"Saved: clean_ai_isot.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe7ciTq5c2rw"
      },
      "source": [
        "##### AI-Dataset 2 - AI_Gen Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "CAZWQXT4oEgD",
        "outputId": "5dc57054-f295-4769-d95b-fe66fb53fb25"
      },
      "outputs": [],
      "source": [
        "print(ai_gen.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "jL6VW1ktc6Hz",
        "outputId": "04aca53b-f751-434a-bc01-b658a451515e"
      },
      "outputs": [],
      "source": [
        "# Keep only needed columns\n",
        "ai_gen = ai_gen[[\n",
        "    \"text\", \"is_misinformation\", \"model_signature\",\n",
        "    \"date\", \"month\", \"country\", \"platform\"\n",
        "]]\n",
        "\n",
        "# Drop rows with missing text\n",
        "ai_gen = ai_gen.dropna(subset=[\"text\"])\n",
        "ai_gen = ai_gen[ai_gen[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "# Clean text\n",
        "ai_gen[\"text\"] = ai_gen[\"text\"].apply(clean_text)\n",
        "\n",
        "# Convert model_signature to AI/Human label\n",
        "# 1 = AI-generated, 0 = Human-written\n",
        "ai_gen = ai_gen[ai_gen[\"model_signature\"].isin([\"GPT-like\", \"human\"])]\n",
        "ai_gen[\"is_ai\"] = ai_gen[\"model_signature\"].apply(\n",
        "    lambda x: 1 if x == \"GPT-like\" else 0\n",
        ")\n",
        "\n",
        "# Convert misinformation column to binary label\n",
        "ai_gen[\"is_real\"] = ai_gen[\"is_misinformation\"].apply(\n",
        "    lambda x: 0 if x == 1 else 1\n",
        ")\n",
        "\n",
        "# Final cleaned AI-gen dataset\n",
        "ai_gen_clean = ai_gen[[\n",
        "    \"text\", \"is_real\", \"is_ai\", \"date\", \"month\", \"country\", \"platform\"\n",
        "]]\n",
        "\n",
        "ai_gen_clean.to_csv(\"clean_ai_gen.csv\", index=False)\n",
        "print(\"Saved: clean_ai_gen.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjhH83rMmUbC"
      },
      "source": [
        "#### Combined Master Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "udI71UVkmYwz",
        "outputId": "5f8421c2-7f95-4d22-9bdb-9ce01ca3641f"
      },
      "outputs": [],
      "source": [
        "combined = pd.concat([df_rf_clean, ai_isot_df, ai_gen], ignore_index=True)\n",
        "\n",
        "combined = combined.drop_duplicates(subset=[\"text\"])\n",
        "combined = combined[combined[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "combined.to_csv(\"combined_master_dataset.csv\", index=False)\n",
        "print(\"Saved: combined_master_dataset.csv\")\n",
        "\n",
        "print(\"Rows in final dataset:\", len(combined))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8WVaRNwPtwx"
      },
      "source": [
        "#### Combine News Datasets Only for EDA:\n",
        "* df_rf_clean\n",
        "* ai_isot_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2p5Gk2OePeEP",
        "outputId": "9b3acbc1-e576-4914-8513-46c886004fb7"
      },
      "outputs": [],
      "source": [
        "combined_news = pd.concat([df_rf_clean, ai_isot_df], ignore_index=True)\n",
        "\n",
        "combined_news = combined.drop_duplicates(subset=[\"text\"])\n",
        "combined_news = combined[combined[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "combined_news.to_csv(\"combined_news_dataset.csv\", index=False)\n",
        "print(\"Saved: combined_news_dataset.csv\")\n",
        "\n",
        "print(\"Rows in Combined News Dataset:\", len(combined_news))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "cPOdmV6xS-yD",
        "outputId": "2b7caa91-02e2-4d95-bf40-1ca306aa5753"
      },
      "outputs": [],
      "source": [
        "combined_news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a290e164"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform an initial data overview of the ***combined_news*** DataFrame by displaying its first few rows, shape, and general information, then analyze and visualize the distribution of the ***is_real*** and ***source*** columns using count plots. Afterwards, handle ***NaN*** values in the ***is_ai*** column by replacing them with -1, and then analyze and visualize its distribution using a count plot. Finally, summarize the key findings from these initial checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28d75bc"
      },
      "source": [
        "#### 1. Initial Data Overview\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "03333b06",
        "outputId": "4580a68f-8313-4410-851f-19f2a1f0f88d"
      },
      "outputs": [],
      "source": [
        "print(\"First 5 rows of combined_news DataFrame:\")\n",
        "print(combined_news.head())\n",
        "\n",
        "print(\"\\nShape of combined_news DataFrame:\")\n",
        "print(combined_news.shape)\n",
        "\n",
        "print(\"\\nGeneral information about combined_news DataFrame:\")\n",
        "combined_news.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2691f556"
      },
      "source": [
        "#### 2. Analyze and visualize the distribution of the ***is_real*** and ***source*** columns using count plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "9c3cf812",
        "outputId": "32b1047d-b009-4c66-e13b-89d43e0d06bb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = ['mediumpurple','gold']\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(data=combined_news, x='is_real', hue='is_real', palette=color, legend = False)\n",
        "plt.title('Distribution of is_real (0=Fake, 1=Real)')\n",
        "plt.xlabel('Is Real')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "c3374c97",
        "outputId": "61195d0b-7a1c-4637-d336-a4e9161751f4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=combined_news, x='source', hue='source', palette=color, legend = False)\n",
        "plt.title('Distribution of Source')\n",
        "plt.xlabel('Source')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29be512e"
      },
      "source": [
        "#### 3. Handle NaN values in the ***is_ai*** column:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Handle `NaN` values in the `is_ai` column by replacing them with -1, and then analyze and visualize its distribution using a count plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8ce35814",
        "outputId": "00f325d3-0174-46fe-8b4e-f01a9122cfaa"
      },
      "outputs": [],
      "source": [
        "combined_news['is_ai'] = combined_news['is_ai'].fillna(-1)\n",
        "\n",
        "print(\"Combined_news DataFrame after handling NaN values in 'is_ai':\")\n",
        "print(combined_news.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "c8b2ca84",
        "outputId": "c182a538-86db-4064-dfb4-c44a305687f7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(data=combined_news, x='is_ai', hue='is_ai', palette=color, legend = False)\n",
        "plt.title('Distribution of is_ai (0=Human, 1=AI, -1=Not applicable)')\n",
        "plt.xlabel('Is AI')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bbff525"
      },
      "source": [
        "#### 4. Summary of Key Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4.1. `combined_news` DataFrame Structure:**\n",
        "*   The DataFrame `combined_news` contains 39,972 entries and 4 columns: `text`, `is_real`, `source`, and `is_ai`.\n",
        "*   The `text`, `is_real`, and `source` columns are fully populated, with no missing values.\n",
        "*   The `is_ai` column initially had a large number of missing (NaN) values, as indicated by only 895 non-null entries before handling.\n",
        "\n",
        "**4.2. Distribution of `is_real`:**\n",
        "*   The count plot for `is_real` shows a fairly balanced distribution between real (1) and fake (0) news entries, which is good for training classification models.\n",
        "\n",
        "**4.3. Distribution of `source`:**\n",
        "*   The `source` column is predominantly composed of entries from \"FAKE-REAL\", with a much smaller proportion from \"AI-ISOT\". This indicates that the dataset is heavily skewed towards traditional fake/real news rather than AI-specific news sources.\n",
        "\n",
        "**4.4. Distribution of `is_ai` after handling NaNs:**\n",
        "*   After replacing `NaN` values with -1, the count plot for `is_ai` clearly shows three categories:\n",
        "    *   `-1` (Not applicable): This is the largest category, representing entries where AI attribution was not originally provided (mostly from the FAKE-REAL dataset).\n",
        "    *   `0` (Human-written): A small number of entries are identified as human-written.\n",
        "    *   `1` (AI-generated): An even smaller number of entries are identified as AI-generated.\n",
        "*   This highlights that the `is_ai` column is largely sparse and mainly applicable to a specific subset of the `combined_news` data (i.e., from the AI-ISOT dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ed9c5b9"
      },
      "source": [
        "#### 5. Text Length and Word Count Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Calculate the number of characters and words for each text entry in the `combined_news` DataFrame. Visualize the distributions of these metrics using histograms or density plots, and compare the average/median text lengths between real (is_real=1) and fake (is_real=0) news.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42261e3f"
      },
      "source": [
        "First, we will calculate the number of characters for each text entry and store it in a new column ***char_count*** in the ***combined_news*** DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "61c6e898",
        "outputId": "10bf91a3-319f-45f8-a961-c5a18ecc53e0"
      },
      "outputs": [],
      "source": [
        "combined_news['char_count'] = combined_news['text'].str.len()\n",
        "print(\"Added 'char_count' column to combined_news DataFrame.\")\n",
        "print(combined_news.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "931897f8"
      },
      "source": [
        "Calculate the number of words for each text entry and store it in a new column ***word_count***.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cb1fb361",
        "outputId": "e5123522-253b-4403-c682-6c34bf6a84cd"
      },
      "outputs": [],
      "source": [
        "combined_news['word_count'] = combined_news['text'].apply(lambda x: len(str(x).split()))\n",
        "print(\"Added 'word_count' column to combined_news DataFrame.\")\n",
        "print(combined_news.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7a0765"
      },
      "source": [
        "Visualize the distribution of character count:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "e2e5bb6c",
        "outputId": "1b859f81-ea85-4d34-e20b-75bc0cc380d2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(combined_news['char_count'], bins=50, kde=True, color='mediumpurple')\n",
        "plt.title('Distribution of Character Count')\n",
        "plt.xlabel('Character Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f873578"
      },
      "source": [
        "Visualize the distribution of the ***word_count***.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "a8595635",
        "outputId": "db26d30b-5920-4cf5-db41-586cec6db1f1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(combined_news['word_count'], bins=50, kde=True, color='gold')\n",
        "plt.title('Distribution of Word Count')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a534241"
      },
      "source": [
        "Calculate the average and median character count for real and fake news articles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "08e47a15",
        "outputId": "bf7d8847-0e4d-43da-96e7-7fd2f8465db3"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAverage and Median Character Count for Real vs. Fake News:\")\n",
        "print(combined_news.groupby('is_real')['char_count'].agg(['mean', 'median']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab772267"
      },
      "source": [
        "Calculate the average and median word count for real and fake news articles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "adb8d2b8",
        "outputId": "4d7d7cb0-01bb-4483-eb8e-9e63df7eeea5"
      },
      "outputs": [],
      "source": [
        "print(\"Average and Median Word Count for Real vs. Fake News:\")\n",
        "print(combined_news.groupby('is_real')['word_count'].agg(['mean', 'median']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3fb88c"
      },
      "source": [
        "Visually compare the ***char_count*** distribution between real and fake news using a box plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "0af1b824",
        "outputId": "cd71927d-8d47-4e03-f03d-aba8d3092ce5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=combined_news, x='is_real', y='char_count', hue='is_real', palette=['mediumpurple', 'gold'], legend=False)\n",
        "plt.title('Character Count Distribution by News Type (0=Fake, 1=Real)')\n",
        "plt.xlabel('News Type')\n",
        "plt.ylabel('Character Count')\n",
        "plt.xticks([0, 1], ['Fake News', 'Real News'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "846d6a7e"
      },
      "source": [
        "Visually compare the ***word_count*** distribution between real and fake news by creating a box plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "3cf2cdaf",
        "outputId": "02cb89d7-bab7-491b-f5c8-5cf70399c537"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=combined_news, x='is_real', y='word_count', hue='is_real', palette=['mediumpurple', 'gold'], legend=False)\n",
        "plt.title('Word Count Distribution by News Type (0=Fake, 1=Real)')\n",
        "plt.xlabel('News Type')\n",
        "plt.ylabel('Word Count')\n",
        "plt.xticks([0, 1], ['Fake News', 'Real News'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add1e058"
      },
      "source": [
        "#### Summary of Text Length and Word Count Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5.1. Character and Word Counts:**\n",
        "*   New columns ***char_count*** and ***word_count*** were successfully added to the ***combined_news*** DataFrame, providing quantitative metrics for text length.\n",
        "\n",
        "**5.2. Distribution of Text Lengths (Character and Word Counts):**\n",
        "*   Both character and word count distributions show a wide range, indicating variability in article lengths within the dataset. Most articles tend to be shorter, with a long tail extending towards very long articles.\n",
        "*   The distributions appear to be right-skewed, meaning there are many shorter articles and fewer very long ones.\n",
        "\n",
        "**5.3. Comparison of Real vs. Fake News (Character Count):**\n",
        "*   **Average Character Count:** Fake news has a slightly higher average character count (approximately 2551) compared to real news (approximately 2424).\n",
        "*   **Median Character Count:** The medians are very close (2281 for fake, 2265 for real), suggesting that for the majority of articles, the character lengths are quite similar.\n",
        "*   The box plots for character count indicate that fake news tends to have a slightly wider spread and potentially more outliers on the higher end, suggesting some fake news articles can be considerably longer.\n",
        "\n",
        "**5.4. Comparison of Real vs. Fake News (Word Count):**\n",
        "*   **Average Word Count:** Similar to character count, fake news articles have a higher average word count (approximately 434) than real news (approximately 398).\n",
        "*   **Median Word Count:** The medians are also close (390 for fake, 372 for real).\n",
        "*   The box plots for word count also show a similar pattern to character count, with fake news potentially having a slightly larger range and more long-form outliers.\n",
        "\n",
        "**Overall Conclusion:**\n",
        "While there isn't a drastic difference, fake news articles in this combined dataset tend to be slightly longer on average in terms of both character and word counts compared to real news articles. This could be a subtle distinguishing feature, although the distributions largely overlap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfa5b67a"
      },
      "source": [
        "#### 6. Word Frequency and Vocabulary Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Extract the most frequent words from the 'text' column. Visualize the top 20 most common words across the entire dataset, and then separately for real and fake news using bar charts or word clouds to identify distinctive vocabulary patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "119c235f",
        "outputId": "9ba61a9f-e52b-4a39-a036-af50b6644494"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "print(\"NLTK stopwords and punkt corpora downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b53863d"
      },
      "source": [
        "Create a text preprocessing function that converts text to lowercase, tokenizes it, removes stopwords, and filters out non-alphabetic tokens, as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ef671f1e",
        "outputId": "e3f4fec2-2df3-4860-f2c4-480ad8cf6b55"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # Convert to string and lowercase\n",
        "    tokens = word_tokenize(text) # Tokenize the text\n",
        "    # Remove stop words and non-alphabetic tokens\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "print(\"Text preprocessing function 'preprocess_text' created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ac43dc0"
      },
      "source": [
        "Now that the preprocessing function is defined, let's apply it to the 'text' column of the ***combined_news*** DataFrame to tokenize and clean the text, storing the result in a new column called 'processed_text'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bd31ddf3",
        "outputId": "48758392-360f-4330-fc9d-99fc0634718f"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab', quiet=True)\n",
        "print(\"NLTK 'punkt_tab' corpus downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a9c3cba5",
        "outputId": "ae04d72d-81f5-4653-a453-c24cc04380d9"
      },
      "outputs": [],
      "source": [
        "combined_news['processed_text'] = combined_news['text'].apply(preprocess_text)\n",
        "print(\"Applied 'preprocess_text' to the 'text' column.\")\n",
        "print(combined_news[['text', 'processed_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82e2e83e"
      },
      "source": [
        "Now that the text is preprocessed and stored as lists of words in the 'processed_text' column, let's combine all these lists into a single flat list to prepare for counting the most frequent words across the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0101974b",
        "outputId": "0e86b701-69f5-4245-a8ed-e076d953dbd8"
      },
      "outputs": [],
      "source": [
        "all_words = [word for sublist in combined_news['processed_text'] for word in sublist]\n",
        "print(f\"Total words in the dataset after preprocessing: {len(all_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65802b37"
      },
      "source": [
        "Use the ***collections.Counter*** to find the 20 most frequent words, which is a required step for visualizing the top common words across the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7a016f8d",
        "outputId": "2ac4b781-2be7-40a9-8e40-075809466583"
      },
      "outputs": [],
      "source": [
        "word_freq = Counter(all_words)\n",
        "top_20_words = word_freq.most_common(20)\n",
        "print(\"Top 20 most common words across the entire dataset:\")\n",
        "print(top_20_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81f91e28"
      },
      "source": [
        "Extract the words and their corresponding frequencies from the ***top_20_words*** list and then use ***matplotlib.pyplot*** and ***seaborn*** to create a bar chart.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "e2403341",
        "outputId": "26ad3f07-614d-4084-a158-f2c1d89f2587"
      },
      "outputs": [],
      "source": [
        "words = [word for word, count in top_20_words]\n",
        "counts = [count for word, count in top_20_words]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x=words, y=counts, hue=words, palette='viridis', legend=False)\n",
        "plt.title('Top 20 Most Common Words Across All News')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Bar plot of top 20 most common words across the entire dataset displayed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9065ee4e"
      },
      "source": [
        "Filter the ***combined_news*** DataFrame to create a separate DataFrame for real news (where ***is_real*** is 1) to analyze its unique vocabulary patterns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7e6883d0",
        "outputId": "659a1776-6065-4332-8a7c-8b7fae6f278e"
      },
      "outputs": [],
      "source": [
        "real_news_df = combined_news[combined_news['is_real'] == 1]\n",
        "print(\"Filtered real news DataFrame created.\")\n",
        "print(f\"Number of real news entries: {len(real_news_df)}\")\n",
        "print(real_news_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8809690b"
      },
      "source": [
        "Combine all the preprocessed words from the ***processed_text*** column into a single list, which is a necessary step before counting the most frequent words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "099484ee",
        "outputId": "f876d3de-c46a-4e89-c47e-a29700226c3e"
      },
      "outputs": [],
      "source": [
        "real_news_words = [word for sublist in real_news_df['processed_text'] for word in sublist]\n",
        "print(f\"Total words in real news dataset: {len(real_news_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31bfa036"
      },
      "source": [
        "Use ***Collections.Counter*** to find the 20 most frequent words specifically for real news.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c7654a4d",
        "outputId": "7e8adc11-a75d-46a8-b30f-0707ba5f3e86"
      },
      "outputs": [],
      "source": [
        "real_word_freq = Counter(real_news_words)\n",
        "top_20_real_words = real_word_freq.most_common(20)\n",
        "print(\"Top 20 most common words in real news:\")\n",
        "print(top_20_real_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee1cd3ea"
      },
      "source": [
        "To visualize the top 20 most common words in real news, I need to extract the words and their corresponding frequencies from the ***top_20_real_words*** list and then create a bar chart.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "8f87c902",
        "outputId": "ed6536ab-7866-4bba-c0c7-12df47bebb83"
      },
      "outputs": [],
      "source": [
        "words_real = [word for word, count in top_20_real_words]\n",
        "counts_real = [count for word, count in top_20_real_words]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x=words_real, y=counts_real, hue=words_real, palette='viridis', legend=False)\n",
        "plt.title('Top 20 Most Common Words in Real News')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Bar plot of top 20 most common words in real news displayed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de69e6e1"
      },
      "source": [
        "Filter the ***combined_news*** DataFrame to create a separate DataFrame for fake news (where ***is_real*** is 0), as specified in the instructions, to analyze its unique vocabulary patterns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b3231e6f",
        "outputId": "0a96e6da-4271-4f54-fe83-4142e89c3aec"
      },
      "outputs": [],
      "source": [
        "fake_news_df = combined_news[combined_news['is_real'] == 0]\n",
        "print(\"Filtered fake news DataFrame created.\")\n",
        "print(f\"Number of fake news entries: {len(fake_news_df)}\")\n",
        "print(fake_news_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "71721dd4",
        "outputId": "ee9c7a32-0bc2-4a49-a925-a7bb346e3bdb"
      },
      "outputs": [],
      "source": [
        "fake_news_words = [word for sublist in fake_news_df['processed_text'] for word in sublist]\n",
        "print(f\"Total words in fake news dataset: {len(fake_news_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbdd1a3d"
      },
      "source": [
        "Now that all processed words from fake news are combined into a single list, let's find the 20 most frequent words specifically for fake news, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0e423ea9",
        "outputId": "b059f975-70a4-48db-cfd3-eb6c1b91d59b"
      },
      "outputs": [],
      "source": [
        "fake_word_freq = Counter(fake_news_words)\n",
        "top_20_fake_words = fake_word_freq.most_common(20)\n",
        "print(\"Top 20 most common words in fake news:\")\n",
        "print(top_20_fake_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "837206cd"
      },
      "source": [
        "#### 7. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform sentiment analysis on the 'text' column to derive polarity (emotional tone, e.g., positive, negative, neutral) and subjectivity scores. Add these as new columns to the DataFrame and visualize their distributions. Compare the sentiment scores between real and fake news to identify potential differences in emotional content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "613ae1f2"
      },
      "source": [
        "Define the ***get_sentiment*** function as specified in the instructions to extract polarity and subjectivity scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3ba07984",
        "outputId": "9cde1416-33b4-4d83-c218-aa05a3da6fa4"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(text):\n",
        "    blob = TextBlob(str(text))\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "print(\"Imported TextBlob and defined 'get_sentiment' function.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3de699d"
      },
      "source": [
        "Now that the ***get_sentiment*** function is defined, let's apply it to the 'text' column of the ***combined_news*** DataFrame to calculate the polarity and subjectivity scores for each text entry. The results will be stored in new columns named 'polarity' and 'subjectivity' respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2246735f",
        "outputId": "d7f02b44-fa92-4042-b876-97b269ac3444"
      },
      "outputs": [],
      "source": [
        "combined_news[['polarity', 'subjectivity']] = combined_news['text'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
        "print(\"Added 'polarity' and 'subjectivity' columns to combined_news DataFrame.\")\n",
        "print(combined_news.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7a3b79a"
      },
      "source": [
        "Visualize its distribution using a histogram or density plot to understand the overall emotional tone of the news articles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "40f4eb8b",
        "outputId": "969a2ee5-3256-4ca1-9ec8-70705eb4d749"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(combined_news['polarity'], bins=50, kde=True, color='mediumpurple')\n",
        "plt.title('Distribution of Polarity Scores')\n",
        "plt.xlabel('Polarity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7541b2f"
      },
      "source": [
        "Now that the 'subjectivity' column has been added to the DataFrame, let's visualize its distribution using a histogram or density plot to understand the degree of factual information versus personal opinions in the news articles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "418fb1bb",
        "outputId": "aa3f3e02-2c68-4eac-d29d-d662d11bc04e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(combined_news['subjectivity'], bins=50, kde=True, color='gold')\n",
        "plt.title('Distribution of Subjectivity Scores')\n",
        "plt.xlabel('Subjectivity Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc17dbb6"
      },
      "source": [
        "Compare the average and median polarity scores between real and fake news articles to identify potential differences in emotional tone.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "49bb88cf",
        "outputId": "ff605959-d95f-4d1c-9a8e-893947097b8b"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAverage and Median Polarity Scores for Real vs. Fake News:\")\n",
        "print(combined_news.groupby('is_real')['polarity'].agg(['mean', 'median']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f154a45"
      },
      "source": [
        "Compare the average and median subjectivity scores between real and fake news articles to identify potential differences in the degree of factual information versus personal opinions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "492a32cc",
        "outputId": "1c810710-7d45-448a-85a2-e72a5c3178cf"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAverage and Median Subjectivity Scores for Real vs. Fake News:\")\n",
        "print(combined_news.groupby('is_real')['subjectivity'].agg(['mean', 'median']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06be76f7"
      },
      "source": [
        "#### Summary of Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Polarity Scores:**\n",
        "*   The distribution of polarity scores across all news articles is centered around zero, indicating a generally neutral emotional tone, with a slight tendency towards positive sentiment.\n",
        "*   There is a relatively wide spread, suggesting a variety of emotional tones present in the dataset.\n",
        "*   **Comparison (Real vs. Fake News):** Both real and fake news articles exhibit very similar average and median polarity scores, indicating that, on average, their emotional tones are quite close. Fake news has a slightly higher average polarity (approx. 0.0598) than real news (approx. 0.0573), but the difference is marginal.\n",
        "\n",
        "**2. Subjectivity Scores:**\n",
        "*   The distribution of subjectivity scores shows a considerable number of articles leaning towards factual (lower subjectivity) content, but also a significant portion with higher subjectivity, indicating opinion-based writing.\n",
        "*   **Comparison (Real vs. Fake News):** A more notable difference is observed in subjectivity scores. Fake news articles have a significantly higher average subjectivity (approx. 0.4549) and median subjectivity (approx. 0.4552) compared to real news articles (average approx. 0.3624, median approx. 0.3676). This suggests that fake news tends to contain more personal opinions and less factual reporting than real news.\n",
        "\n",
        "**Overall Conclusion:**\n",
        "Sentiment analysis reveals a clearer distinction in subjectivity than in polarity. While both real and fake news exhibit similar emotional tones, fake news articles are generally more subjective, implying a higher presence of opinions, beliefs, and personal feelings compared to the more objective nature of real news. This difference in subjectivity could be a valuable feature for distinguishing between the two news types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41e65839"
      },
      "source": [
        "#### Text Analysis Summary:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Key Findings:\n",
        "\n",
        "*   **Text Length Comparison:** Fake news articles tend to be slightly longer than real news articles on average. The average character count for fake news was approximately 2551, while for real news it was around 2424. Similarly, the average word count for fake news was about 434, compared to approximately 398 for real news. However, median lengths were very similar, and distributions largely overlapped, suggesting that while there's a slight tendency, length isn't a strong discriminator on its own.\n",
        "*   **Word Frequency Patterns:**\n",
        "    *   \"Trump\" is a highly frequent word across all news types, appearing as the most common word in fake and overall news, and the second most common in real news.\n",
        "    *   Real news frequently features journalistic terms like \"said\" and source attributions like \"reuters\" (28,861 occurrences in real news).\n",
        "    *   Fake news frequently uses words like \"people,\" \"president,\" \"one,\" \"donald,\" \"like,\" \"obama,\" \"clinton,\" \"video,\" and \"hillary,\" suggesting a focus on specific figures, personal opinions, and potentially sensational content.\n",
        "*   **Sentiment Polarity:** Both real and fake news exhibit very similar emotional tones, with average polarity scores being approximately 0.0573 for real news and 0.0598 for fake news. The distributions for polarity are centered around zero, indicating a generally neutral to slightly positive emotional tone for both categories.\n",
        "*   **Sentiment Subjectivity:** A significant difference was observed in subjectivity. Fake news articles are notably more subjective, with an average subjectivity score of approximately 0.4549 and a median of 0.4552. In contrast, real news articles are more objective, with an average subjectivity score of approximately 0.3624 and a median of 0.3676. This indicates that fake news often contains more opinions and beliefs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MODELS and ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Long Short-Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLaoVsFIvRMK"
      },
      "source": [
        "#### BERT Modeling and Integration Part 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZzTT4h3FsQp"
      },
      "source": [
        "This section builds on existing EDA work and uses the **combined_news** dataset\n",
        "we have already prepared, which includes:\n",
        "- Real/Fake news (df_rf_clean)\n",
        "- AI-ISOT news dataset (ai_isot_df)\n",
        "\n",
        "MEMORY OPTIMIZATION STRATEGIES:\n",
        "- Using DistilBERT (lighter than BERT)\n",
        "- Gradient accumulation for effective larger batches\n",
        "- Reduced max sequence length to 256 tokens\n",
        "- Memory cleanup between stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "g9bwCe4JFlFR",
        "outputId": "09bdf883-950d-41af-8d3b-3bb7c4939184"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CHECK DATA SIZE AND SAMPLE IF NEEDED\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"Original combined_news size: {len(combined_news)} samples\")\n",
        "\n",
        "# IMPORTANT: Sample the data to speed up training\n",
        "# Adjust SAMPLE_SIZE:\n",
        "# - 5000 = ~30-45 min total training time (RECOMMENDED for testing)\n",
        "# - 10000 = ~1-2 hours\n",
        "# - 20000 = ~3-4 hours\n",
        "# - None = use all data (8+ hours)\n",
        "\n",
        "SAMPLE_SIZE = 5000  # Start small to test!\n",
        "\n",
        "if SAMPLE_SIZE and len(combined_news) > SAMPLE_SIZE:\n",
        "    print(f\"âš ï¸  Sampling {SAMPLE_SIZE} samples for faster training...\")\n",
        "    combined_news_sampled = combined_news.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "    print(f\"âœ“ Using {len(combined_news_sampled)} samples\")\n",
        "\n",
        "    # Replace combined_news with the sampled version\n",
        "    combined_news = combined_news_sampled\n",
        "else:\n",
        "    print(f\"âœ“ Using all {len(combined_news)} samples\")\n",
        "\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(combined_news['is_real'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3527
        },
        "id": "hY4IcdH5vMt5",
        "outputId": "40fd1b6f-d32b-4d05-ca79-a74618054b96"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BERT MODELING SETUP - OPTIMIZED FOR GOOGLE COLAB\n",
        "# ============================================================================\n",
        "\n",
        "# Install NEW required libraries only (pandas, numpy, matplotlib already imported)\n",
        "!pip install transformers torch scikit-learn bertviz\n",
        "\n",
        "# Import garbage collection for memory management\n",
        "import gc\n",
        "\n",
        "# Import NEW libraries for BERT (not already in your notebook)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available and set device accordingly\n",
        "# GPU acceleration significantly speeds up training (10-100x faster)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# ============================================================================\n",
        "# CUSTOM DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Creating a custom PyTorch Dataset class for our news text data.\n",
        "This class handles:\n",
        "- Text tokenization (converting words to numbers BERT understands)\n",
        "- Padding/truncation to fixed length\n",
        "- Creating attention masks (tells model which tokens are real vs padding)\n",
        "- Returning data in the format PyTorch expects\n",
        "\"\"\"\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Gets a single sample from the dataset.\n",
        "        This is called by DataLoader during training.\n",
        "\n",
        "        Returns a dictionary containing:\n",
        "        - input_ids: Token IDs for BERT\n",
        "        - attention_mask: Mask indicating real tokens vs padding\n",
        "        - labels: The true label for this sample\n",
        "        \"\"\"\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text using BERT tokenizer\n",
        "        # This converts words to numerical IDs that BERT understands\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,      # Adds [CLS] and [SEP] tokens\n",
        "            max_length=self.max_length,    # Truncate longer sequences\n",
        "            padding='max_length',          # Pad shorter sequences\n",
        "            truncation=True,               # Truncate if too long\n",
        "            return_attention_mask=True,    # Create attention mask\n",
        "            return_tensors='pt'            # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 1: REAL/FAKE CLASSIFIER\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "STAGE 1 OVERVIEW:\n",
        "This is our first classifier that determines if a news article is REAL or FAKE.\n",
        "\n",
        "Why two stages?\n",
        "- Stage 1: Focuses on authenticity (is this real news?)\n",
        "- Stage 2: Focuses on authorship (was this written by AI?)\n",
        "\n",
        "This hierarchical approach allows each model to specialize on its specific task.\n",
        "\n",
        "We're using the combined_news dataset that includes:\n",
        "- Real and Fake news articles\n",
        "- News with AI/Human labels from AI-ISOT dataset\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 1: Training REAL/FAKE News Classifier\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use the combined_news DataFrame that's already in your notebook\n",
        "# This combines df_rf_clean and ai_isot_df\n",
        "print(f\"Using combined_news DataFrame with {len(combined_news)} samples\")\n",
        "print(f\"Class distribution:\\n{combined_news['is_real'].value_counts()}\")\n",
        "\n",
        "# Prepare data for Stage 1 (Real vs Fake classification)\n",
        "# We only need the 'text' and 'is_real' columns\n",
        "df_stage1 = combined_news[['text', 'is_real']].dropna().copy()\n",
        "\n",
        "print(f\"\\nAfter removing NaN values: {len(df_stage1)} samples\")\n",
        "\n",
        "# Split data into train (70%), validation (15%), and test (15%)\n",
        "# Stratify ensures balanced class distribution in all splits\n",
        "X_train_s1, X_temp_s1, y_train_s1, y_temp_s1 = train_test_split(\n",
        "    df_stage1['text'].values,\n",
        "    df_stage1['is_real'].values,\n",
        "    test_size=0.3,           # 30% for val + test\n",
        "    random_state=42,         # For reproducibility\n",
        "    stratify=df_stage1['is_real'].values  # Balanced split\n",
        ")\n",
        "\n",
        "# Split the 30% into validation and test (15% each)\n",
        "X_val_s1, X_test_s1, y_val_s1, y_test_s1 = train_test_split(\n",
        "    X_temp_s1, y_temp_s1,\n",
        "    test_size=0.5,           # 50% of 30% = 15% of total\n",
        "    random_state=42,\n",
        "    stratify=y_temp_s1\n",
        ")\n",
        "\n",
        "print(f\"\\nData Split:\")\n",
        "print(f\"  Train: {len(X_train_s1)} samples\")\n",
        "print(f\"  Validation: {len(X_val_s1)} samples\")\n",
        "print(f\"  Test: {len(X_test_s1)} samples\")\n",
        "\n",
        "# Initialize the BERT tokenizer for Stage 1\n",
        "# This converts text to tokens that DistilBERT can process\n",
        "tokenizer_s1 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Create PyTorch datasets for train, validation, and test\n",
        "# These wrap our data and handle tokenization\n",
        "train_dataset_s1 = NewsDataset(X_train_s1, y_train_s1, tokenizer_s1)\n",
        "val_dataset_s1 = NewsDataset(X_val_s1, y_val_s1, tokenizer_s1)\n",
        "test_dataset_s1 = NewsDataset(X_test_s1, y_test_s1, tokenizer_s1)\n",
        "\n",
        "# Create DataLoaders\n",
        "# DataLoaders handle batching, shuffling, and parallel data loading\n",
        "BATCH_SIZE = 8  # Small batch size for memory optimization\n",
        "train_loader_s1 = DataLoader(train_dataset_s1, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_s1 = DataLoader(val_dataset_s1, batch_size=BATCH_SIZE)\n",
        "test_loader_s1 = DataLoader(test_dataset_s1, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"\\nBatch Configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Training batches: {len(train_loader_s1)}\")\n",
        "print(f\"  Validation batches: {len(val_loader_s1)}\")\n",
        "print(f\"  Test batches: {len(test_loader_s1)}\")\n",
        "\n",
        "# Initialize the DistilBERT model for sequence classification\n",
        "# We're using a pre-trained model and fine-tuning it on our data\n",
        "model_s1 = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',  # Pre-trained model name\n",
        "    num_labels=2                 # Binary classification (Real vs Fake)\n",
        ").to(device)                     # Move model to GPU if available\n",
        "\n",
        "# Training hyperparameters\n",
        "# These control how the model learns\n",
        "EPOCHS = 2           # Number of passes through the dataset\n",
        "LEARNING_RATE = 2e-5 # How big of steps to take during optimization (standard for BERT)\n",
        "ACCUMULATION_STEPS = 2  # Gradient accumulation for effective batch size of 16\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Accumulation Steps: {ACCUMULATION_STEPS}\")\n",
        "print(f\"  Effective Batch Size: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
        "\n",
        "# Set up optimizer (AdamW is specifically designed for transformers)\n",
        "optimizer_s1 = AdamW(model_s1.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Set up learning rate scheduler with warmup\n",
        "# This gradually increases learning rate at start, then decreases\n",
        "# Helps with training stability and convergence\n",
        "total_steps = len(train_loader_s1) * EPOCHS\n",
        "scheduler_s1 = get_linear_schedule_with_warmup(\n",
        "    optimizer_s1,\n",
        "    num_warmup_steps=0,           # No warmup steps\n",
        "    num_training_steps=total_steps # Total training steps\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Training function that handles one complete pass through the training data.\n",
        "\n",
        "For each batch:\n",
        "1. Forward pass: Get predictions from model\n",
        "2. Calculate loss: How wrong were the predictions?\n",
        "3. Backward pass: Calculate gradients\n",
        "4. Optimizer step: Update model weights (with gradient accumulation)\n",
        "5. Track metrics: Loss and accuracy\n",
        "\"\"\"\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device, accumulation_steps=1):\n",
        "    \"\"\"Train for one epoch with gradient accumulation for memory efficiency\"\"\"\n",
        "    # Set model to training mode (enables dropout, batch norm updates, etc.)\n",
        "    model.train()\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Progress bar for visual feedback during training\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Move data to GPU\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass: get model predictions\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        # Extract loss and predictions\n",
        "        loss = outputs.loss / accumulation_steps  # Normalize loss for accumulation\n",
        "        logits = outputs.logits  # Raw prediction scores\n",
        "\n",
        "        # Backward pass: calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Only update weights every accumulation_steps\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            # Clip gradients to prevent exploding gradients problem\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            # Zero out gradients for next accumulation\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # Convert logits to predictions (highest score wins)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({'loss': loss.item() * accumulation_steps})\n",
        "\n",
        "        # Clear some memory periodically\n",
        "        del input_ids, attention_mask, labels, outputs, loss, logits\n",
        "        if batch_idx % 100 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions.double() / total_predictions\n",
        "\n",
        "    return avg_loss, accuracy.item()\n",
        "\n",
        "def eval_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation or test set.\n",
        "\n",
        "    No gradient calculation needed (saves memory and speeds up evaluation).\n",
        "    Returns predictions, true labels, and confidence scores.\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode (disables dropout, etc.)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    confidences = []\n",
        "\n",
        "    # No gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Move data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass only\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert logits to probabilities using softmax\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "            # Get predicted class and confidence score\n",
        "            conf, preds = torch.max(probs, dim=1)\n",
        "\n",
        "            # Store predictions for analysis\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            confidences.extend(conf.cpu().numpy())\n",
        "\n",
        "            # Clean up\n",
        "            del input_ids, attention_mask, labels, outputs, loss, logits, probs\n",
        "\n",
        "    # Clear cache after evaluation\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    return avg_loss, accuracy, predictions, true_labels, confidences\n",
        "\n",
        "# ============================================================================\n",
        "# TRAIN STAGE 1 MODEL\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Now we'll train the Stage 1 model through multiple epochs.\n",
        "Each epoch involves:\n",
        "1. Training on the training set\n",
        "2. Evaluating on the validation set\n",
        "3. Tracking metrics to monitor progress\n",
        "\n",
        "We save the best model and track learning curves.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING STAGE 1 MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Lists to store metrics for plotting learning curves\n",
        "train_losses_s1 = []\n",
        "val_losses_s1 = []\n",
        "train_accs_s1 = []\n",
        "val_accs_s1 = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Train for one epoch with gradient accumulation\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model_s1, train_loader_s1, optimizer_s1, scheduler_s1, device, ACCUMULATION_STEPS\n",
        "    )\n",
        "\n",
        "    # Validate after each epoch\n",
        "    val_loss, val_acc, _, _, _ = eval_model(model_s1, val_loader_s1, device)\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses_s1.append(train_loss)\n",
        "    val_losses_s1.append(val_loss)\n",
        "    train_accs_s1.append(train_acc)\n",
        "    val_accs_s1.append(val_acc)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"\\nðŸ“Š Epoch {epoch + 1} Results:\")\n",
        "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "    # Memory cleanup after each epoch\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Save the trained model for later use\n",
        "torch.save(model_s1.state_dict(), 'model_stage1_real_fake.pth')\n",
        "print(\"\\nâœ… Stage 1 model saved as 'model_stage1_real_fake.pth'\")\n",
        "\n",
        "# Memory cleanup before evaluation\n",
        "del train_loader_s1, val_loader_s1\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATE ON TEST SET\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Final evaluation on the held-out test set.\n",
        "This gives us an unbiased estimate of model performance.\n",
        "\n",
        "We'll generate:\n",
        "- Overall accuracy\n",
        "- Classification report (precision, recall, F1 for each class)\n",
        "- Confusion matrix visualization\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATING STAGE 1 ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_loss_s1, test_acc_s1, test_preds_s1, test_labels_s1, test_conf_s1 = eval_model(\n",
        "    model_s1, test_loader_s1, device\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Stage 1 Test Results:\")\n",
        "print(f\"   Test Loss: {test_loss_s1:.4f}\")\n",
        "print(f\"   Test Accuracy: {test_acc_s1:.4f}\")\n",
        "print(f\"\\nðŸ“‹ Classification Report:\")\n",
        "print(classification_report(test_labels_s1, test_preds_s1,\n",
        "                          target_names=['Fake', 'Real']))\n",
        "\n",
        "# Plot confusion matrix to see where the model makes mistakes\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm_s1 = confusion_matrix(test_labels_s1, test_preds_s1)\n",
        "sns.heatmap(cm_s1, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fake', 'Real'],\n",
        "            yticklabels=['Fake', 'Real'])\n",
        "plt.title('Stage 1: Real/Fake Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZE TRAINING HISTORY\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Plot learning curves to understand training dynamics:\n",
        "- Left plot: Loss over epochs (should decrease)\n",
        "- Right plot: Accuracy over epochs (should increase)\n",
        "\n",
        "If validation metrics get worse while training improves, that's overfitting.\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_losses_s1, label='Train Loss', marker='o', linewidth=2)\n",
        "ax1.plot(val_losses_s1, label='Val Loss', marker='s', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Loss', fontsize=12)\n",
        "ax1.set_title('Stage 1: Training and Validation Loss', fontsize=14)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "ax2.plot(train_accs_s1, label='Train Accuracy', marker='o', linewidth=2)\n",
        "ax2.plot(val_accs_s1, label='Val Accuracy', marker='s', linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "ax2.set_title('Stage 1: Training and Validation Accuracy', fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# MEMORY CLEANUP BEFORE STAGE 2\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Clean up memory before moving to Stage 2.\n",
        "This is crucial for preventing crashes in Google Colab.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MEMORY CLEANUP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clear unnecessary variables\n",
        "print(\"Cleaning up Stage 1 variables...\")\n",
        "del train_dataset_s1, val_dataset_s1\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"âœ“ GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"âœ“ GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\nâœ… Stage 1 complete! Ready for Stage 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB9fXdzAHPqY"
      },
      "source": [
        "##### Stage 1 (Real/Fake Classifier) Phase Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This stage focused on building a robust classifier to distinguish between real and fake news articles. It served as the primary filter in our two-stage classification pipeline. The key steps and findings were:\n",
        "\n",
        "*   **Purpose:** The main objective of Stage 1 was to accurately classify incoming news articles as either 'Real' (1) or 'Fake' (0).\n",
        "*   **Data Sampling:** To optimize training time and resource utilization, especially within the Colab environment, the combined_news dataset was sampled down to 5000 entries. This allowed for quicker iteration and experimentation while still providing a sufficiently large dataset for meaningful training. The class distribution of the sampled data was verified to ensure balance.\n",
        "*   **Model Setup:** We utilized DistilBertTokenizer and DistilBertForSequenceClassification from the transformers library. DistilBERT was chosen for its balance of performance and efficiency, being lighter than full BERT models. The tokenizer was initialized with distilbert-base-uncased, and the model was configured for binary classification (num_labels=2) and moved to the available computing device (GPU if available, otherwise CPU).\n",
        "*   **Data Splitting:** The sampled data (df_stage1) was meticulously split into training (70%), validation (15%), and test (15%) sets using train_test_split. Crucially, stratify was employed to ensure that the proportion of real and fake news was maintained across all splits, preventing class imbalance issues during training and evaluation.\n",
        "*   **Training Process:** The model was trained over 2 EPOCHS with a LEARNING_RATE of 2e-5. A BATCH_SIZE of 8 was used in conjunction with ACCUMULATION_STEPS = 2, effectively simulating an effective batch size of 16 to manage memory while still achieving good gradient updates. An AdamW optimizer and a linear scheduler with warmup were employed. The custom train_epoch function handled the training loop, including gradient clipping and periodic memory cleanup. The eval_model function performed evaluations on the validation set after each epoch.\n",
        "*   **Performance on Test Set:** The Stage 1 model demonstrated exceptional performance on the held-out test set:\n",
        "    *   **Test Accuracy:** 0.9933 (99.33%)\n",
        "    *   **Classification Report:** Showed high precision, recall, and F1-scores for both 'Fake' and 'Real' classes, indicating robust classification across both categories.\n",
        "    *   **Confusion Matrix:** Revealed very few misclassifications, with the model accurately identifying the vast majority of real and fake news articles.\n",
        "*   **Training History Visualization:** Plots of training and validation loss and accuracy curves were generated. These showed a clear decrease in loss and an increase in accuracy for both sets over the epochs, suggesting good learning without significant overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1921
        },
        "id": "CLB0vGhehYiN",
        "outputId": "6e0ca8cd-003e-4946-91ac-150ae74ff76e"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STAGE 2: HUMAN/AI CLASSIFIER (STREAMLINED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 2: Training HUMAN/AI Classifier\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Memory cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Load Stage 2 data - only samples with VALID is_ai labels (0 or 1, not -1)\n",
        "df_stage2 = combined_news[combined_news['is_ai'].notna()][['text', 'is_ai']].copy()\n",
        "\n",
        "# CRITICAL FIX: Remove -1 values (which mean \"Not applicable\")\n",
        "print(f\"Before filtering: {len(df_stage2)} samples\")\n",
        "print(f\"is_ai distribution:\\n{df_stage2['is_ai'].value_counts()}\")\n",
        "\n",
        "df_stage2 = df_stage2[df_stage2['is_ai'].isin([0, 1])].dropna()\n",
        "\n",
        "print(f\"\\nAfter filtering: {len(df_stage2)} samples\")\n",
        "print(f\"Class distribution:\\n{df_stage2['is_ai'].value_counts()}\")\n",
        "\n",
        "# If there's no data for Stage 2, skip it\n",
        "if len(df_stage2) == 0:\n",
        "    print(\"\\nâš ï¸  No AI/Human labeled data available for Stage 2!\")\n",
        "    print(\"Skipping Stage 2 training...\")\n",
        "\n",
        "    # Create a dummy classifier that always returns None for Stage 2\n",
        "    class TwoStageClassifierStage1Only:\n",
        "        def __init__(self, model_s1, tokenizer_s1, device):\n",
        "            self.model_s1 = model_s1\n",
        "            self.tokenizer_s1 = tokenizer_s1\n",
        "            self.device = device\n",
        "            self.model_s1.eval()\n",
        "\n",
        "        def predict(self, text, max_length=256):\n",
        "            encoding_s1 = self.tokenizer_s1.encode_plus(\n",
        "                text, add_special_tokens=True, max_length=max_length,\n",
        "                padding='max_length', truncation=True,\n",
        "                return_attention_mask=True, return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids_s1 = encoding_s1['input_ids'].to(self.device)\n",
        "            attention_mask_s1 = encoding_s1['attention_mask'].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs_s1 = self.model_s1(input_ids=input_ids_s1, attention_mask=attention_mask_s1)\n",
        "                probs_s1 = torch.softmax(outputs_s1.logits, dim=1)\n",
        "                conf_s1, pred_s1 = torch.max(probs_s1, dim=1)\n",
        "\n",
        "            is_real = pred_s1.item()\n",
        "            real_confidence = conf_s1.item()\n",
        "\n",
        "            return {\n",
        "                'is_real': is_real,\n",
        "                'real_confidence': real_confidence,\n",
        "                'real_label': 'Real' if is_real == 1 else 'Fake',\n",
        "                'is_ai': None,\n",
        "                'ai_confidence': None,\n",
        "                'ai_label': 'N/A (Stage 2 not trained)'\n",
        "            }\n",
        "\n",
        "        def predict_batch(self, texts):\n",
        "            return [self.predict(text) for text in texts]\n",
        "\n",
        "    two_stage_classifier = TwoStageClassifierStage1Only(model_s1, tokenizer_s1, device)\n",
        "    print(\"âœ… Using Stage 1 only classifier\")\n",
        "\n",
        "else:\n",
        "    # Continue with normal Stage 2 training...\n",
        "    # Split data\n",
        "    X_train_s2, X_temp_s2, y_train_s2, y_temp_s2 = train_test_split(\n",
        "        df_stage2['text'].values,\n",
        "        df_stage2['is_ai'].values,\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=df_stage2['is_ai'].values\n",
        "    )\n",
        "\n",
        "    X_val_s2, X_test_s2, y_val_s2, y_test_s2 = train_test_split(\n",
        "        X_temp_s2, y_temp_s2,\n",
        "        test_size=0.5,\n",
        "        random_state=42,\n",
        "        stratify=y_temp_s2\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(X_train_s2)}, Val: {len(X_val_s2)}, Test: {len(X_test_s2)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    tokenizer_s2 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    train_dataset_s2 = NewsDataset(X_train_s2, y_train_s2.astype(int), tokenizer_s2)\n",
        "    val_dataset_s2 = NewsDataset(X_val_s2, y_val_s2.astype(int), tokenizer_s2)\n",
        "    test_dataset_s2 = NewsDataset(X_test_s2, y_test_s2.astype(int), tokenizer_s2)\n",
        "\n",
        "    # Create loaders\n",
        "    train_loader_s2 = DataLoader(train_dataset_s2, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader_s2 = DataLoader(val_dataset_s2, batch_size=BATCH_SIZE)\n",
        "    test_loader_s2 = DataLoader(test_dataset_s2, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize model\n",
        "    model_s2 = DistilBertForSequenceClassification.from_pretrained(\n",
        "        'distilbert-base-uncased',\n",
        "        num_labels=2\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer_s2 = AdamW(model_s2.parameters(), lr=LEARNING_RATE)\n",
        "    total_steps_s2 = len(train_loader_s2) * EPOCHS\n",
        "    scheduler_s2 = get_linear_schedule_with_warmup(\n",
        "        optimizer_s2,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps_s2\n",
        "    )\n",
        "\n",
        "    # Train Stage 2\n",
        "    print(\"\\nTraining Stage 2...\")\n",
        "    train_losses_s2 = []\n",
        "    val_losses_s2 = []\n",
        "    train_accs_s2 = []\n",
        "    val_accs_s2 = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model_s2, train_loader_s2, optimizer_s2, scheduler_s2, device, ACCUMULATION_STEPS\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, _, _, _ = eval_model(model_s2, val_loader_s2, device)\n",
        "\n",
        "        train_losses_s2.append(train_loss)\n",
        "        val_losses_s2.append(val_loss)\n",
        "        train_accs_s2.append(train_acc)\n",
        "        val_accs_s2.append(val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model_s2.state_dict(), 'model_stage2_human_ai.pth')\n",
        "    print(\"\\nâœ… Stage 2 model saved\")\n",
        "\n",
        "    # Cleanup\n",
        "    del train_loader_s2, val_loader_s2\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Evaluate on test\n",
        "    print(\"\\nEvaluating Stage 2...\")\n",
        "    test_loss_s2, test_acc_s2, test_preds_s2, test_labels_s2, test_conf_s2 = eval_model(\n",
        "        model_s2, test_loader_s2, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStage 2 Test Accuracy: {test_acc_s2:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels_s2, test_preds_s2, target_names=['Human', 'AI']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm_s2 = confusion_matrix(test_labels_s2, test_preds_s2)\n",
        "    sns.heatmap(cm_s2, annot=True, fmt='d', cmap='Greens',\n",
        "                xticklabels=['Human', 'AI'],\n",
        "                yticklabels=['Human', 'AI'])\n",
        "    plt.title('Stage 2: Human/AI Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================================================\n",
        "    # TWO-STAGE CLASSIFIER\n",
        "    # ============================================================================\n",
        "\n",
        "    class TwoStageClassifier:\n",
        "        \"\"\"Two-stage news classifier: Real/Fake â†’ Human/AI\"\"\"\n",
        "\n",
        "        def __init__(self, model_s1, model_s2, tokenizer_s1, tokenizer_s2, device):\n",
        "            self.model_s1 = model_s1\n",
        "            self.model_s2 = model_s2\n",
        "            self.tokenizer_s1 = tokenizer_s1\n",
        "            self.tokenizer_s2 = tokenizer_s2\n",
        "            self.device = device\n",
        "            self.model_s1.eval()\n",
        "            self.model_s2.eval()\n",
        "\n",
        "        def predict(self, text, max_length=256):\n",
        "            # Stage 1: Real/Fake\n",
        "            encoding_s1 = self.tokenizer_s1.encode_plus(\n",
        "                text, add_special_tokens=True, max_length=max_length,\n",
        "                padding='max_length', truncation=True,\n",
        "                return_attention_mask=True, return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids_s1 = encoding_s1['input_ids'].to(self.device)\n",
        "            attention_mask_s1 = encoding_s1['attention_mask'].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs_s1 = self.model_s1(input_ids=input_ids_s1, attention_mask=attention_mask_s1)\n",
        "                probs_s1 = torch.softmax(outputs_s1.logits, dim=1)\n",
        "                conf_s1, pred_s1 = torch.max(probs_s1, dim=1)\n",
        "\n",
        "            is_real = pred_s1.item()\n",
        "            real_confidence = conf_s1.item()\n",
        "\n",
        "            result = {\n",
        "                'is_real': is_real,\n",
        "                'real_confidence': real_confidence,\n",
        "                'real_label': 'Real' if is_real == 1 else 'Fake',\n",
        "                'is_ai': None,\n",
        "                'ai_confidence': None,\n",
        "                'ai_label': 'N/A'\n",
        "            }\n",
        "\n",
        "            # Stage 2: Human/AI (only if Fake)\n",
        "            if is_real == 0:\n",
        "                encoding_s2 = self.tokenizer_s2.encode_plus(\n",
        "                    text, add_special_tokens=True, max_length=max_length,\n",
        "                    padding='max_length', truncation=True,\n",
        "                    return_attention_mask=True, return_tensors='pt'\n",
        "                )\n",
        "\n",
        "                input_ids_s2 = encoding_s2['input_ids'].to(self.device)\n",
        "                attention_mask_s2 = encoding_s2['attention_mask'].to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs_s2 = self.model_s2(input_ids=input_ids_s2, attention_mask=attention_mask_s2)\n",
        "                    probs_s2 = torch.softmax(outputs_s2.logits, dim=1)\n",
        "                    conf_s2, pred_s2 = torch.max(probs_s2, dim=1)\n",
        "\n",
        "                is_ai = pred_s2.item()\n",
        "                ai_confidence = conf_s2.item()\n",
        "\n",
        "                result['is_ai'] = is_ai\n",
        "                result['ai_confidence'] = ai_confidence\n",
        "                result['ai_label'] = 'AI-generated' if is_ai == 1 else 'Human-written'\n",
        "\n",
        "            return result\n",
        "\n",
        "        def predict_batch(self, texts):\n",
        "            return [self.predict(text) for text in texts]\n",
        "\n",
        "    # Initialize classifier\n",
        "    two_stage_classifier = TwoStageClassifier(\n",
        "        model_s1, model_s2, tokenizer_s1, tokenizer_s2, device\n",
        "    )\n",
        "\n",
        "    print(\"\\nâœ… Two-Stage Classifier ready!\")\n",
        "\n",
        "# ============================================================================\n",
        "# DEMO (works for both Stage 1 only and full two-stage)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEMO: Two-Stage Classification\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test on 3 examples\n",
        "for i in range(min(3, len(X_test_s1))):\n",
        "    text = X_test_s1[i]\n",
        "    result = two_stage_classifier.predict(text)\n",
        "\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: {text[:150]}...\")\n",
        "    print(f\"Stage 1: {result['real_label']} ({result['real_confidence']:.2%})\")\n",
        "    if result['is_ai'] is not None:\n",
        "        print(f\"Stage 2: {result['ai_label']} ({result['ai_confidence']:.2%})\")\n",
        "\n",
        "# Quick evaluation on subset\n",
        "test_subset = X_test_s1[:20]\n",
        "labels_subset = y_test_s1[:20]\n",
        "\n",
        "results = two_stage_classifier.predict_batch(test_subset)\n",
        "correct = sum(1 for i, r in enumerate(results) if r['is_real'] == labels_subset[i])\n",
        "\n",
        "print(f\"\\nâœ… Test Accuracy (20 samples): {correct/len(test_subset):.2%}\")\n",
        "print(\"\\nâœ… Part 2 Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klSDsQ3hHn26"
      },
      "source": [
        "##### Stage 2 (Human/AI Classifier) Phase Summary\n",
        "\n",
        "This stage was designed to further classify news articles identified as 'Fake' by Stage 1, determining if they were human-written or AI-generated. The approach depended on the availability of sufficient labeled data:\n",
        "\n",
        "*   **Purpose:** The primary goal of Stage 2 was to classify news as 'Human-written' (0) or 'AI-generated' (1), but only for articles that had already been classified as 'Fake' by the Stage 1 model.\n",
        "*   **Data Preparation:** The data for Stage 2 (df_stage2) was meticulously filtered from the combined_news DataFrame to include only samples with valid is_ai labels (i.e., 0.0 or 1.0), explicitly excluding the -1.0 (Not applicable) entries. This ensured that only relevant data for AI/human classification was used.\n",
        "*   **Conditional Training:** Due to the sample size setting (SAMPLE_SIZE = 5000), the filtered df_stage2 contained only 142 samples (89 human-written, 53 AI-generated). While this is a small dataset for training a complex BERT model, the training process proceeded to demonstrate the full two-stage pipeline.\n",
        "*   **Training Process (if applicable):** Similar to Stage 1, DistilBertTokenizer and DistilBertForSequenceClassification were used. The data was split into training (99 samples), validation (21 samples), and test (22 samples) sets, stratified to maintain class distribution. The model was trained for 2 EPOCHS using AdamW and a linear scheduler, with BATCH_SIZE = 8 and ACCUMULATION_STEPS = 2.\n",
        "*   **Performance on Test Set:**\n",
        "    *   **Test Accuracy:** The Stage 2 model achieved a test accuracy of 0.6364 (63.64%).\n",
        "    *   **Classification Report:** The classification report showed a precision of 0.64 for the 'Human' class, but 0.00 for the 'AI' class. This indicates that while the model correctly identified most human-written examples, it struggled significantly with identifying AI-generated content, often misclassifying it as human-written. This is reflected in the recall for 'Human' being 1.00 (all human examples were predicted as human) and for 'AI' being 0.00 (no AI examples were predicted as AI).\n",
        "    *   **Confusion Matrix:** The confusion matrix visually confirmed these results, showing that all 14 true human-written samples were correctly classified, but all 8 true AI-generated samples were incorrectly classified as human-written. This highlights a significant limitation likely due to the small size and potential imbalance of the Stage 2 training data.\n",
        "*   **Two-Stage Classifier Integration:** Both trained models (Stage 1 and Stage 2) were integrated into a TwoStageClassifier. This classifier first uses model_s1 to predict if an article is real or fake. If it's classified as fake, model_s2 is then used to determine if it's human-written or AI-generated. This hierarchical structure allows for specialized classification at each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2914
        },
        "id": "Qh8H_PNxi3YY",
        "outputId": "bb2c4f9e-982c-4126-afdd-399e0e34d51c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ATTENTION VISUALIZATION AND EXPLAINABILITY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ATTENTION VISUALIZATION & EXPLAINABILITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "# ============================================================================\n",
        "# ATTENTION EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class DistilBertWithAttention:\n",
        "    \"\"\"Extract attention weights from DistilBERT\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, device):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_attention(self, text, max_length=256):\n",
        "        \"\"\"Extract attention weights and tokens\"\"\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_attentions=True\n",
        "            )\n",
        "\n",
        "        return outputs.attentions, tokens\n",
        "\n",
        "# Initialize for Stage 1\n",
        "attention_viz_s1 = DistilBertWithAttention(tokenizer_s1, device)\n",
        "print(\"âœ“ Attention visualizer ready\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def plot_attention_heatmap(attention, tokens, layer=0, head=0, max_tokens=40):\n",
        "    \"\"\"Plot attention heatmap for specific layer/head\"\"\"\n",
        "    attn_matrix = attention[layer][0, head].cpu().numpy()\n",
        "\n",
        "    # Get non-padding tokens\n",
        "    non_pad_idx = min(max_tokens, len([t for t in tokens if t != '[PAD]']))\n",
        "    attn_matrix = attn_matrix[:non_pad_idx, :non_pad_idx]\n",
        "    tokens_display = tokens[:non_pad_idx]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        attn_matrix,\n",
        "        xticklabels=tokens_display,\n",
        "        yticklabels=tokens_display,\n",
        "        cmap='viridis',\n",
        "        cbar=True\n",
        "    )\n",
        "    plt.title(f'Attention Heatmap - Layer {layer}, Head {head}')\n",
        "    plt.xlabel('Key Tokens')\n",
        "    plt.ylabel('Query Tokens')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_token_importance(attention, tokens):\n",
        "    \"\"\"Calculate importance score for each token\"\"\"\n",
        "    all_attentions = torch.stack([attn[0] for attn in attention])\n",
        "    importance = all_attentions.sum(dim=(0, 1, 2)).cpu().numpy()\n",
        "\n",
        "    token_importance = {}\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "            token_importance[token] = importance[i]\n",
        "\n",
        "    return token_importance\n",
        "\n",
        "def plot_token_importance(token_importance, top_n=15, title=\"Token Importance\"):\n",
        "    \"\"\"Plot top N most important tokens\"\"\"\n",
        "    sorted_tokens = sorted(token_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_tokens = sorted_tokens[:top_n]\n",
        "\n",
        "    tokens = [t[0] for t in top_tokens]\n",
        "    scores = [t[1] for t in top_tokens]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(len(tokens)), scores, color='steelblue')\n",
        "    plt.yticks(range(len(tokens)), tokens)\n",
        "    plt.xlabel('Importance Score')\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def explain_prediction(text, stage='stage1'):\n",
        "    \"\"\"Complete explainability analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"EXPLAINING PREDICTION - {stage.upper()}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    attention_viz = attention_viz_s1\n",
        "\n",
        "    print(f\"\\nText: {text[:200]}...\")\n",
        "\n",
        "    # Get prediction\n",
        "    result = two_stage_classifier.predict(text)\n",
        "    print(f\"\\nPrediction: {result['real_label']}\")\n",
        "    print(f\"Confidence: {result['real_confidence']:.2%}\")\n",
        "\n",
        "    # Get attention\n",
        "    attention, tokens = attention_viz.get_attention(text)\n",
        "\n",
        "    # Visualizations\n",
        "    print(\"\\nðŸ“Š Attention Heatmap (Layer 0, Head 0):\")\n",
        "    plot_attention_heatmap(attention, tokens, layer=0, head=0)\n",
        "\n",
        "    print(\"\\nðŸ“Š Most Important Tokens:\")\n",
        "    token_importance = get_token_importance(attention, tokens)\n",
        "    plot_token_importance(token_importance, top_n=15, title=\"Top Important Tokens\")\n",
        "\n",
        "    return {\n",
        "        'attention': attention,\n",
        "        'tokens': tokens,\n",
        "        'prediction': result,\n",
        "        'token_importance': token_importance\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# DEMO: EXPLAIN PREDICTIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEMO: EXPLAINING MODEL DECISIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Analyze 2 examples from test set\n",
        "for idx in [0, 5]:\n",
        "    if idx < len(X_test_s1):\n",
        "        example_text = X_test_s1[idx]\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EXAMPLE {idx + 1}\")\n",
        "        results = explain_prediction(example_text, stage='stage1')\n",
        "\n",
        "print(\"\\nâœ… Explainability analysis complete!\")\n",
        "print(\"\\nYou can now use: explain_prediction(your_text, stage='stage1')\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY STATISTICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nðŸ“Š Stage 1 (Real/Fake Classifier):\")\n",
        "print(f\"   Test Accuracy: {test_acc_s1:.2%}\")\n",
        "print(f\"   Training Samples: {len(X_train_s1)}\")\n",
        "\n",
        "if 'test_acc_s2' in locals():\n",
        "    print(f\"\\nðŸ“Š Stage 2 (Human/AI Classifier):\")\n",
        "    print(f\"   Test Accuracy: {test_acc_s2:.2%}\")\n",
        "    print(f\"   Training Samples: {len(X_train_s2)}\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Stage 2: Not trained (no AI/Human labeled data)\")\n",
        "\n",
        "print(f\"\\nâœ… Models saved:\")\n",
        "print(f\"   - model_stage1_real_fake.pth\")\n",
        "if 'model_s2' in locals():\n",
        "    print(f\"   - model_stage2_human_ai.pth\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ‰ PROJECT COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4686509c"
      },
      "source": [
        "##### Attention Visualization and Explainability Phase Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This phase was critical for understanding *how* the DistilBERT model arrived at its predictions, rather than just *what* its predictions were. By peering into the model's internal workings, we gained insights into which parts of the text were most influential in its decision-making process. The steps involved were:\n",
        "\n",
        "*   **Attention Extraction:** A custom DistilBertWithAttention class was created to facilitate the extraction of attention weights directly from the DistilBertModel (a base DistilBERT model, not the fine-tuned classifier). This class tokenized the input text and then performed a forward pass through the model with output_attentions=True to retrieve the attention matrices for all layers and heads.\n",
        "*   **Visualization Functions:**\n",
        "    *   **Attention Heatmaps:** The plot_attention_heatmap function was developed to visualize the attention patterns within specific layers and heads of the model. These heatmaps illustrate how each token in a sequence attends to every other token, providing a visual representation of token-level relationships the model learned. For example, a heatmap would show if the model strongly associated certain keywords with the overall classification.\n",
        "    *   **Token Importance Plots:** The get_token_importance function aggregated attention weights across all layers and heads to calculate an overall 'importance score' for each token. Tokens with higher scores were deemed more critical to the model's understanding of the text. The plot_token_importance function then visualized the top N most important tokens using a bar chart, clearly highlighting the words that caught the model's attention the most.\n",
        "*   **Explain Prediction Function:** An explain_prediction function was implemented to integrate these capabilities. Given a text input, it first generated a prediction using the two-stage classifier, and then used the attention extraction and visualization functions to display the attention heatmap (for a specific layer/head) and the most important tokens. This provided a comprehensive explanation of the model's reasoning for a given input.\n",
        "*   **Insights from Visualizations:** The attention visualizations provided concrete examples of words and phrases that significantly influenced the model's predictions. For instance, in real news articles, journalistic elements like city names (e.g., 'brussels', 'jerusalem') and news agency mentions (e.g., 'reuters') might show high importance, aligning with their factual nature. In fake news, highly charged emotional words or specific political figures' names could register as more important, reflecting the potentially sensational or opinionated content. These visual insights corroborated the findings from the EDA phase, particularly regarding word frequency and sentiment subjectivity, demonstrating how the model leverages these linguistic features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b284502"
      },
      "source": [
        "##### Summary Analysis and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This project successfully developed a two-stage classification system using DistilBERT to distinguish between real/fake and human/AI-generated news. The phased approach allowed for specialized model training and clearer insights into different aspects of news authenticity and authorship.\n",
        "\n",
        "**Overall Assessment:**\n",
        "*   **Stage 1 (Real/Fake Classifier):** Achieved outstanding performance with a test accuracy of **99.33%**. This model is highly effective at identifying whether a news article is real or fake, making it a robust initial filter.\n",
        "*   **Stage 2 (Human/AI Classifier):** Demonstrated limited success with a test accuracy of **63.64%**, primarily due to its inability to correctly classify AI-generated content. The classification report and confusion matrix clearly showed that while the model performed well for human-written content (perfect recall for the Human class), it entirely missed AI-generated samples (zero recall for the AI class).\n",
        "\n",
        "**Strengths and Limitations:**\n",
        "*   **Strength - Stage 1:** The Real/Fake classifier is a significant success, proving that DistilBERT, even with sampled data, can generalize well to this task. The high F1-scores across both classes indicate balanced and reliable performance.\n",
        "*   **Limitation - Stage 2:** The Human/AI classifiers poor performance for AI-generated content is a major limitation. This is likely attributable to the extremely small size of the AI-labeled dataset (only 142 samples, leading to just 22 test samples), which is insufficient for effective fine-tuning of a complex model like DistilBERT. The imbalance within this small dataset (89 human, 53 AI) further exacerbated the issue.\n",
        "\n",
        "**Alignment with EDA and Explainability:**\n",
        "*   **EDA Insights:** The EDA highlighted key differences: fake news was slightly longer and significantly more subjective than real news. While Stage 1s high accuracy suggests it leveraged these and other subtle linguistic cues effectively, the direct impact of these specific features on the models decision-making was further elucidated by attention visualization.\n",
        "*   **Explainability:** The attention heatmaps and token importance plots provided valuable insights. They confirmed that the model focused on critical keywords, journalistic markers (like reuters in real news), and emotionally charged terms (potentially in fake news). This explainability layer helps build trust in the models predictions for Stage 1.\n",
        "\n",
        "**Challenges and Future Improvements:**\n",
        "1.  **Data Scarcity for Stage 2:** The primary challenge was the limited availability of high-quality, diverse AI-generated news data. A larger, more balanced dataset for distinguishing human from AI-generated text would drastically improve Stage 2s performance.\n",
        "2.  **Model Complexity vs. Data Size:** Fine-tuning DistilBERT on only 142 samples for Stage 2 is suboptimal. Future work could explore:\n",
        "    *   **Data Augmentation:** Techniques to generate more AI-labeled text.\n",
        "    *   **Transfer Learning:** Freezing more layers of DistilBERT for Stage 2 if data remains scarce.\n",
        "    *   **Simpler Models:** Using less complex models for Stage 2 if the feature space is very distinct.\n",
        "3.  **Refining Explainability:** While attention is a good start, more advanced explainability techniques (e.g., LIME, SHAP) could offer deeper insights into feature contributions beyond just token importance.\n",
        "4.  **Real-world Deployment:** For deployment, consider optimizing the two-stage classifier for inference speed and handling edge cases where a text might fall ambiguously between categories.\n",
        "\n",
        "In conclusion, the project successfully built a highly accurate real/fake news detection system (Stage 1). However, the human/AI classification (Stage 2) requires substantial data augmentation or a re-evaluation of its modeling approach to become equally effective. The combination of strong predictive models and interpretability tools offers a promising framework for combating misinformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Baseline Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import kagglehub #import module\n",
        "import pandas as pd #import module\n",
        "import os #import module\n",
        "import re #import module\n",
        "import numpy as np #import module\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV #import module\n",
        "from sklearn.pipeline import Pipeline #import module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer #import module\n",
        "from sklearn.linear_model import LogisticRegression #import module\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay #import module\n",
        "import matplotlib.pyplot as plt #import module\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5) #sets parameters\n",
        "\n",
        "#download dataset\n",
        "path_fake_real = kagglehub.dataset_download(\n",
        "    \"clmentbisaillon/fake-and-real-news-dataset\"\n",
        ")\n",
        "\n",
        "#shows where dataset was downloaded in folder\n",
        "print(\"fake/real path:\", path_fake_real)\n",
        "print(\"Contents of fake/real path:\", os.listdir(path_fake_real))\n",
        "\n",
        "#download dataset\n",
        "path_ai1 = kagglehub.dataset_download(\n",
        "    \"walidbenaouda/ai-isot-dataset\"\n",
        ")\n",
        "\n",
        "#shows where dataset was downloaded in folder\n",
        "print(\"ai1 path:\", path_ai1)\n",
        "print(\"Contents of ai1 path:\", os.listdir(path_ai1))\n",
        "\n",
        "#download dataset\n",
        "path_ai2 = kagglehub.dataset_download(\n",
        "    \"atharvasoundankar/gen-ai-misinformation-detection-datase-20242025\"\n",
        ")\n",
        "\n",
        "#shows where dataset was downloaded in folder\n",
        "print(\"ai2 path:\", path_ai2)\n",
        "print(\"Contents of ai2 path:\", os.listdir(path_ai2))\n",
        "\n",
        "fake = pd.read_csv(os.path.join(path_fake_real, \"Fake.csv\")) #cvs for fake\n",
        "true = pd.read_csv(os.path.join(path_fake_real, \"True.csv\")) #cvs for true\n",
        "\n",
        "#cvs for Ai misinformation\n",
        "ai = pd.read_csv(os.path.join(\n",
        "    path_ai2,\n",
        "    \"generative_ai_misinformation_dataset.csv\"\n",
        "))\n",
        "\n",
        "print(fake.head()) #shows headers for fake\n",
        "print(true.head()) #shows headers for true\n",
        "print(ai.head()) #shows headers for AI misinformation\n",
        "\n",
        "#cleans text in dataset\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text) #gets rid of urls\n",
        "    text = re.sub(r\"<.*?>\", \"\", text) #gets rid of html\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?'â€™\\s]\", \" \", text) #makes sure punctuation still there\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() #fixes spaces\n",
        "    return text\n",
        "\n",
        "fake[\"label\"] = 0 #gives fake news a label\n",
        "true[\"label\"] = 1 #gives true news a label\n",
        "\n",
        "df_rf = pd.concat([fake, true], ignore_index=True) #combines datasets\n",
        "\n",
        "#makes text in one place\n",
        "df_rf[\"text\"] = (\n",
        "    df_rf[\"title\"].fillna(\"\") + \" \" +\n",
        "    df_rf[\"text\"].fillna(\"\")\n",
        ").str.strip()\n",
        "\n",
        "#gets rid of duplicates\n",
        "#gets rid of empty text\n",
        "df_rf = df_rf.drop_duplicates(subset=[\"text\"])\n",
        "df_rf = df_rf.dropna(subset=[\"text\"])\n",
        "df_rf = df_rf[df_rf[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "#cleans the text\n",
        "df_rf[\"text\"] = df_rf[\"text\"].apply(clean_text)\n",
        "\n",
        "#picks the necessary columns\n",
        "df_rf_clean = df_rf[[\"text\", \"label\"]]\n",
        "\n",
        "#saves the cleaned file\n",
        "df_rf_clean.to_csv(\"clean_real_fake.csv\", index=False)\n",
        "\n",
        "#shows it was saved\n",
        "print(\"saved: clean_real_fake.csv\")\n",
        "\n",
        "#shows headers of cleaned dataset\n",
        "df_rf_clean.head()\n",
        "\n",
        "#picks the necessary columns\n",
        "ai = ai[[\"text\", \"is_misinformation\", \"timestamp\", \"country\"]].copy()\n",
        "\n",
        "#renames columns\n",
        "ai = ai.rename(columns={\"is_misinformation\": \"label\"})\n",
        "\n",
        "#gets rid of duplicates\n",
        "#gets rid of empty text\n",
        "ai = ai.drop_duplicates(subset=[\"text\"])\n",
        "ai = ai.dropna(subset=[\"text\"])\n",
        "ai = ai[ai[\"text\"].str.strip() != \"\"]\n",
        "\n",
        "#cleans the text\n",
        "ai[\"text\"] = ai[\"text\"].apply(clean_text)\n",
        "\n",
        "#saves the cleaned file\n",
        "ai.to_csv(\"clean_ai_human.csv\", index=False)\n",
        "\n",
        "#shows it was saved\n",
        "print(\"saved: clean_ai_human.csv\")\n",
        "\n",
        "#shows headers of cleaned dataset\n",
        "ai.head()\n",
        "\n",
        "#uses copy of cleaned dataset for training\n",
        "df = df_rf_clean.copy()\n",
        "\n",
        "X = df[\"text\"] #labels features from article text\n",
        "y = df[\"label\"] #labels target for fake and real\n",
        "\n",
        "#splits into the sets for train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "#changes text to tf-idf & logistic regression\n",
        "tfidf_log_reg_pipeline = Pipeline([\n",
        "    (\"vec\", TfidfVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        max_df=.9\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=500,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "#setting parameters for what we are searching for\n",
        "tfidf_param_grid = {\n",
        "    \"vec__ngram_range\": [(1, 1), (1, 2)],\n",
        "    \"clf__C\": [.01, 1]\n",
        "}\n",
        "\n",
        "#how the search actually works\n",
        "tfidf_grid = GridSearchCV(\n",
        "    tfidf_log_reg_pipeline,\n",
        "    param_grid=tfidf_param_grid,\n",
        "    cv=3,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "#runs the gridsearch\n",
        "tfidf_grid.fit(X_train, y_train)\n",
        "\n",
        "#shows best hyperparameters\n",
        "print(\"\\n=== tf-idf + logistic regression ===\")\n",
        "print(\"best tf-idf params:\", tfidf_grid.best_params_)\n",
        "print(\"best cv score:\", tfidf_grid.best_score_)\n",
        "\n",
        "#sets prediction labels for test set\n",
        "y_pred_tfidf = tfidf_grid.predict(X_test)\n",
        "print(\"\\ntf-idf logistic regression - test set report\")\n",
        "print(classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "#makes confusion matrix plot\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_tfidf)\n",
        "plt.title(\"tf-idf + logistic regression - confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "#bow & logisic regression\n",
        "bow_log_reg_pipeline = Pipeline([\n",
        "    (\"vec\", CountVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        max_df=.9\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=500,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "#setting parameters for what we are searching for\n",
        "bow_param_grid = {\n",
        "    \"vec__ngram_range\": [(1, 1)],\n",
        "    \"vec__min_df\": [2],\n",
        "    \"clf__C\": [.01],\n",
        "    \"clf__class_weight\": [None, \"balanced\"]\n",
        "}\n",
        "\n",
        "#how the search actually works\n",
        "bow_grid = GridSearchCV(\n",
        "    bow_log_reg_pipeline,\n",
        "    param_grid=bow_param_grid,\n",
        "    cv=3,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "#runs gridsearch for bow model\n",
        "bow_grid.fit(X_train, y_train)\n",
        "\n",
        "#shows best hyperparameters\n",
        "print(\"\\n=== bow + logistic regression ===\")\n",
        "print(\"best bow params:\", bow_grid.best_params_)\n",
        "print(\"best cv score:\", bow_grid.best_score_)\n",
        "\n",
        "#sets prediction labels for test set\n",
        "y_pred_bow = bow_grid.predict(X_test)\n",
        "print(\"\\nbow logistic regression - test set report\")\n",
        "print(classification_report(y_test, y_pred_bow))\n",
        "\n",
        "#shows confusion matrix graph\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_bow)\n",
        "plt.title(\"bow + logistic regression - confusion matrix\")\n",
        "plt.show()\n",
        "\n",
        "#gets date & label columns in real/fake dataset\n",
        "df_trend = df_rf[[\"date\", \"label\"]].copy()\n",
        "\n",
        "#gets rid of duplicates\n",
        "#gets rid of empty text\n",
        "df_trend[\"date\"] = pd.to_datetime(\n",
        "    df_trend[\"date\"],\n",
        "    errors=\"coerce\",\n",
        "    dayfirst=True\n",
        ")\n",
        "\n",
        "#gets rid of rows where date isn't readable\n",
        "df_trend = df_trend.dropna(subset=[\"date\"])\n",
        "\n",
        "#groups dates by first of month\n",
        "df_trend[\"month\"] = df_trend[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "#shows headers and columns\n",
        "print(df_trend.head())\n",
        "print(df_trend.columns)\n",
        "\n",
        "#groups by month & labels\n",
        "trend_over_time = (\n",
        "    df_trend.groupby([\"month\", \"label\"])   # <-- FIX\n",
        "        .size()\n",
        "        .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "#shows headers\n",
        "print(trend_over_time.head())\n",
        "\n",
        "#makes linechart for news count over time\n",
        "plt.figure(figsize=(10, 5))\n",
        "for lab in trend_over_time[\"label\"].unique():\n",
        "    subset = trend_over_time[trend_over_time[\"label\"] == lab]\n",
        "    plt.plot(subset[\"month\"], subset[\"count\"], marker=\"o\", label=str(lab))\n",
        "\n",
        "plt.title(\"news count over time\") #titles chart\n",
        "plt.xlabel(\"month\") #labels xaxis\n",
        "plt.ylabel(\"count\") #labels yaxis\n",
        "plt.legend() #makes legend\n",
        "plt.xticks(rotation=45) #adjusts month labels\n",
        "plt.tight_layout() #adjusts layout\n",
        "plt.show() #shows linechart\n",
        "\n",
        "#makes ai_trend with labels from cleaned dataset\n",
        "ai_trend = ai[[\"timestamp\", \"country\", \"label\"]].copy()\n",
        "\n",
        "#gets timestamps\n",
        "ai_trend[\"timestamp\"] = pd.to_datetime(ai_trend[\"timestamp\"])\n",
        "\n",
        "#gets dates\n",
        "ai_trend[\"date\"] = ai_trend[\"timestamp\"].dt.date\n",
        "\n",
        "#changes back to datetime\n",
        "ai_trend[\"date\"] = pd.to_datetime(ai_trend[\"date\"])\n",
        "\n",
        "#makes groupings by month\n",
        "ai_trend[\"month\"] = ai_trend[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "#sets patameters\n",
        "def is_us_country(x):\n",
        "    if isinstance(x, str):\n",
        "        x_upper = x.upper()\n",
        "        return (\"USA\" in x_upper) or (\"UNITED STATES\" in x_upper) or (x_upper == \"US\")\n",
        "    return False\n",
        "\n",
        "#makes region as US & international\n",
        "ai_trend[\"region\"] = np.where(ai_trend[\"country\"].apply(is_us_country),\n",
        "                              \"US\", \"International\")\n",
        "\n",
        "#makes region, month & labels\n",
        "#finds number of posts\n",
        "#counts the found total\n",
        "region_trend = (\n",
        "    ai_trend.groupby([\"month\", \"region\", \"label\"])\n",
        "            .size()\n",
        "            .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "#shows headers\n",
        "print(region_trend.head())\n",
        "\n",
        "#keeps only misinformation posts labeled\n",
        "misinfo_region_trend = region_trend[region_trend[\"label\"] == 1]\n",
        "\n",
        "#shows headers\n",
        "print(misinfo_region_trend.head())\n",
        "\n",
        "#makes linechart for time series based on region\n",
        "plt.figure(figsize=(10, 5))\n",
        "for region in misinfo_region_trend[\"region\"].unique():\n",
        "  subset = misinfo_region_trend[misinfo_region_trend[\"region\"] == region]\n",
        "  plt.plot(subset[\"month\"], subset[\"count\"], marker=\"o\", label=region)\n",
        "\n",
        "\n",
        "plt.title(\"misinformation over time: US vs international\") #titles chart\n",
        "plt.xlabel(\"month\") #labels xaxis\n",
        "plt.ylabel(\"number of misinformation posts\") #labels yaxis\n",
        "plt.legend() #makes legend\n",
        "plt.xticks(rotation=45) #adjusts month labels\n",
        "plt.tight_layout() #adjusts layout\n",
        "plt.show() #shows linegraph\n",
        "\n",
        "#installs lime\n",
        "!pip install lime\n",
        "\n",
        "from lime.lime_text import LimeTextExplainer #import module\n",
        "\n",
        "#gets best model from tf-idf from gridsearch\n",
        "best_model = tfidf_grid.best_estimator_\n",
        "\n",
        "#sort labels listed in training data\n",
        "label_values = sorted(y_train.unique())\n",
        "\n",
        "#change labels to strings for lime\n",
        "class_names = [str(l) for l in label_values]\n",
        "\n",
        "#shows class names for lime\n",
        "print(\"class names for lime:\", class_names)\n",
        "\n",
        "#makes lime explainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "#picks text example to explain\n",
        "idx = 10\n",
        "\n",
        "#text of data\n",
        "text_instance = X_test.iloc[idx]\n",
        "\n",
        "#true label\n",
        "true_label = y_test.iloc[idx]\n",
        "\n",
        "#shows text from article\n",
        "#shows true label\n",
        "#shows models predicted label\n",
        "print(\"=== TEXT EXAMPLE ==\")\n",
        "print(text_instance)\n",
        "print(\"\\ntrue label:\", true_label)\n",
        "print(\"model prediction:\", best_model.predict([text_instance])[0])\n",
        "\n",
        "#text we want to explain\n",
        "exp = explainer.explain_instance(\n",
        "    text_instance,\n",
        "    best_model.predict_proba,\n",
        "    num_features=10,\n",
        "    top_labels=1\n",
        ")\n",
        "\n",
        "#shos explanation with highlighted words\n",
        "exp.show_in_notebook(text=True)\n",
        "\n",
        "#saves file as html\n",
        "exp.save_to_file(\"lime_example_1.html\")\n",
        "\n",
        "#shows file has been saved\n",
        "print(\"saved lime explanation to lime_example_1.html\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
